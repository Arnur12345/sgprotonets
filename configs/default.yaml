# SGProtoNet Default Configuration
# All hyperparameters with defaults. Override in experiment-specific YAML files.

seed: 42

# Model architecture
model:
  d_model: 192
  d_visual: 768
  d_semantic: 768
  d_hidden: 384

  visual_encoder:
    name: "google/vit-base-patch16-224"  # ViT-B/16 pretrained on ImageNet-21k
    type: "vit"  # "biomedclip" | "vit"
    freeze: true
    unfreeze_last_n_blocks: 0

  semantic_encoder:
    name: "google-bert/bert-base-uncased"  # Standard BERT (generic, not biomedical)
    type: "bert"  # "biomedclip" | "pubmedbert" | "biobert" | "bert"
    freeze: true
    unfreeze_last_n_blocks: 0
    max_length: 256

  sgam:
    num_heads: 4
    dropout: 0.1

  fusion:
    dropout: 0.1

  vis2sem:
    d_hidden: 512

  distance: "cosine"  # "cosine" | "euclidean"
  prototype_mode: "vanilla"  # "vanilla" | "semantic_weighted"

# Few-shot episode settings
episode:
  n_way: 5
  k_shot: 1
  q_query: 8  # Reduce to 1-2 if using rare classes (edema, hernia, fibrosis, pleural_thickening)

# Data
data:
  dataset: "iu_xray"
  data_dir: "data/processed/"
  image_size: 224
  # Class splits — defined per dataset
  # Using only classes with sufficient samples (>=9 for k_shot=1, q_query=8)
  # For proper 5-way episodes, each split needs 5 classes
  train_classes: ["effusion", "cardiomegaly", "infiltrate", "pneumothorax", "emphysema"]
  val_classes: ["mass", "nodule", "pneumonia", "atelectasis", "consolidation"]
  test_classes: []  # Not enough rare classes with sufficient samples - use val for now

# Training
training:
  # Mixed precision training
  use_amp: true

  # Phase 1: Modality Alignment
  phase1:
    enabled: true
    num_epochs: 20
    batch_size: 16
    lr: 1e-4
    weight_decay: 1e-4

  # Phase 2: Episodic Meta-Training
  phase2:
    num_epochs: 40
    episodes_per_epoch: 250
    lr: 1e-4
    weight_decay: 1e-4
    lambda_align: 0.5
    lambda_consist: 0.1

  optimizer: "adamw"
  scheduler:
    name: "cosine"  # "cosine" | "step"
    warmup_epochs: 5
    min_lr: 1e-6

  # Evaluation during training
  val_episodes: 200
  val_every_n_epochs: 10

  # Checkpointing
  checkpoint_dir: "checkpoints/"
  save_best: true
  save_every_n_epochs: 10

# Logging
logging:
  use_wandb: false
  wandb_project: "sgprotonet"
  wandb_entity: null
  log_every_n_steps: 10

# Inference
inference:
  text_strategy: "class_anchors"  # "class_anchors" | "vis2sem" | "visual_only"
  num_eval_episodes: 600
  confidence_interval: 0.95

# Multi-label binary decomposition settings
# Enable with: multilabel.enabled=true
multilabel:
  enabled: false  # Set to true to use multi-label mode with binary decomposition

  # Binary episode configuration
  n_labels: 5      # Number of labels per episode (L)
  k_pos: 3         # Positive support examples per label
  k_neg: 5         # Negative support examples per label
  q_query: 10      # Query examples per label (balanced pos/neg)

  # Adaptive anchor blending for positive prototypes
  # Weights are adaptive based on number of positive samples:
  #   >20 samples: α=0.3 (trust visual prototype)
  #   10-20 samples: α=0.5 (balanced)
  #   5-10 samples: α=0.7 (rely on semantic anchor)
  #   <5 samples: α=0.9 (almost pure semantic anchor)
  anchor_weight_adaptive: true

  # Loss configuration
  use_focal_loss: true    # Focal loss handles extreme label imbalance
  focal_gamma: 2.0        # Focusing parameter (higher = more focus on hard examples)
  focal_alpha: 0.25       # Balance parameter for positive/negative
  lambda_margin: 0.1      # Weight for prototype margin loss

  # Evaluation threshold
  threshold: 0.5          # Sigmoid threshold for binary predictions

  # Sampling constraints
  min_positives: 3        # Minimum positive samples to include a label in episodes
  min_negatives: 10       # Minimum negative samples
